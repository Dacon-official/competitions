{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Dacon] AI프렌즈 시즌2 강수량 산출 경진대회\n",
    "## giba.kim (팀명)\n",
    "## 2020년 5월 29일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN 기반 Custom Model Single\n",
    "\n",
    ">* Data: Original Data, -9999 제거, 추가 데이터 사용 약 8300개 (-9999제거, 위 경도가 심하게 일치하지 않은 것 제외)\n",
    "* Cross Validation: train_test_split(sorted(glob.glob(train_path + '/*')), test_size=0.1, random_state=31014)\n",
    "* Loss: MOFLoss\n",
    "* Optimizer: RAdam + LARS + LookAHead (https://github.com/mgrankin/over9000)\n",
    "* Scheduler: CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6)\n",
    "* Model: CNN 기반 Custom Model\n",
    "* Batch: 128\n",
    "* Epoch: 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler,AdamW\n",
    "from torchvision import transforms,models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "RAdam + LARS + LookAHead\n",
    "\n",
    "Lookahead implementation from https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n",
    "RAdam + LARS implementation from https://gist.github.com/redknightlois/c4023d393eb8f92bb44b2ab582d7ec20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        # manually add our defaults to the param groups\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if 'slow_buffer' not in param_state:\n",
    "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
    "                param_state['slow_buffer'].copy_(fast_p.data)\n",
    "            slow = param_state['slow_buffer']\n",
    "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # print(self.k)\n",
    "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group['lookahead_step'] += 1\n",
    "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict['state']\n",
    "        param_groups = fast_state_dict['param_groups']\n",
    "        return {\n",
    "            'state': fast_state,\n",
    "            'slow_state': slow_state,\n",
    "            'param_groups': param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            'state': state_dict['state'],\n",
    "            'param_groups': state_dict['param_groups'],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "\n",
    "        # We want to restore the slow state, but share param_groups reference\n",
    "        # with base_optimizer. This is a bit redundant but least code\n",
    "        slow_state_new = False\n",
    "        if 'slow_state' not in state_dict:\n",
    "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
    "            state_dict['slow_state'] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
    "        if slow_state_new:\n",
    "            # reapply defaults to catch missing lookahead specific ones\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)\n",
    "                    \n",
    "class Ralamb(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(Ralamb, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ralamb, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = radam_step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                radam_step = p_data_fp32.clone()\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
    "\n",
    "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
    "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
    "                if weight_norm == 0 or radam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / radam_norm\n",
    "\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = radam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "\n",
    "                if N_sma >= 5:\n",
    "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대회 Metric Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    _fs = fscore(y_true, y_pred)\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    print(\"F-Score: \", _fs)\n",
    "    print(\"MAE: \", _mae)\n",
    "    return _mae / (_fs + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value fix\n",
    "# seed 값을 고정해야 hyper parameter 바꿀 때마다 결과를 비교할 수 있습니다.\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 0\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, train_files, is_test_or_not=False, is_transform=False, aug_ratio=0):\n",
    "        \"\"\"\n",
    "        train_files: train file list\n",
    "        is_test_or_not: test or not\n",
    "        is_transform: True augmentation\n",
    "        aug_ratio: augmentation ratio\n",
    "        \"\"\"\n",
    "        self.train_files = train_files\n",
    "        self.is_test_or_not=is_test_or_not\n",
    "        self.is_transform = is_transform\n",
    "        self.aug_ratio = aug_ratio\n",
    "    \n",
    "    # data augmenation 6개\n",
    "    # rot90, rot180, rot270, vflip, hflip, transpose\n",
    "    def aug_flip(self, feature, target):\n",
    "        switch = np.random.choice(6) # random 선택\n",
    "        if switch==0: # rot90\n",
    "            feature_aug=np.rot90(feature,k=1,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=1,axes=[1,2]).copy()  \n",
    "        elif switch==1: # rot180\n",
    "            feature_aug=np.rot90(feature,k=2,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=2,axes=[1,2]).copy()\n",
    "        elif switch==2:  # rot270\n",
    "            feature_aug=np.rot90(feature,k=3,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=3,axes=[1,2]).copy()\n",
    "        elif switch==3:  # vflip\n",
    "            feature_aug=np.flip(feature,axis=[1]).copy()\n",
    "            target_aug=np.flip(target,axis=[1]).copy()\n",
    "        elif switch==4:  # hflip\n",
    "            feature_aug=np.flip(feature,axis=[2]).copy()\n",
    "            target_aug=np.flip(target,axis=[2]).copy()\n",
    "        elif switch==5:  # transpose\n",
    "            feature_aug=np.transpose(feature,[0,2,1]).copy()\n",
    "            target_aug=np.transpose(target,[0,2,1]).copy()\n",
    "\n",
    "        return feature_aug, target_aug\n",
    "\n",
    "    def augmetation(self, feature, target):\n",
    "        # uniform한 확률분포를 가져와서 지정된 aug_ratio보다 작으면 augmentation을 하지 않습니다.\n",
    "        aug_prop = np.random.uniform()\n",
    "        is_aug = aug_prop <= self.aug_ratio\n",
    "        if not is_aug:\n",
    "            return feature, target\n",
    "        \n",
    "        feature_aug, target_aug = self.aug_flip(feature, target)\n",
    "            \n",
    "        return feature_aug, target_aug\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            dataset = np.load(self.train_files[i]) \n",
    "        except:\n",
    "            dataset = np.load(self.train_files[i].replace('\\\\','/')) \n",
    "        \n",
    "        # target 값 정의\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        \n",
    "        # train일 때 target값은 제외하고 전처리 수행하기 위해서\n",
    "        if not self.is_test_or_not:\n",
    "            dataset = dataset[:,:,:-1]\n",
    "        \n",
    "        # GMI, DPR의 위/경도 diff\n",
    "        dataset[:,:,10] = dataset[:,:,10] - dataset[:,:,12]\n",
    "        dataset[:,:,11] = dataset[:,:,11] - dataset[:,:,13]\n",
    "        \n",
    "        # StandardScaling\n",
    "        norm_temp = (dataset[:,:,:12]-mean_vector[None,None,:12])/std_vector[None,None,:12]\n",
    "        feature = np.moveaxis(norm_temp,-1,0).astype(np.float32)\n",
    "        \n",
    "        # test와 augmentation을 하지 않을 경우 그대로 image와 target return\n",
    "        if self.is_test_or_not or not self.is_transform:\n",
    "            return feature, target\n",
    "        \n",
    "        # augmentation 수행하고 return\n",
    "        return self.augmetation(feature, target)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validset batch size를 2로하면 mae_over_fscore가 계산이 잘 안되는 문제가 있어 validset 전체에 대하여 Metric을 계산하는 Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mof_valid(nn_model, valid_loader):\n",
    "    print(\"Valid All MOF\")\n",
    "    nn_model.eval()\n",
    "    val_results = []\n",
    "    val_targets = []\n",
    "    for batch_idx, (feature, target) in enumerate(valid_loader):\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        target_copy = copy.deepcopy(target)\n",
    "        val_results.append(nn_model.predict(feature_copy.cuda()).view(-1,1600).cpu().numpy())\n",
    "        val_targets.append(target_copy.view(-1,1600).cpu().numpy())\n",
    "        del feature\n",
    "        del target\n",
    "    mof = maeOverFscore(np.concatenate(val_targets),np.concatenate(val_results))\n",
    "    print(mof)\n",
    "    return mof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. train feature파일을 load하여 강수량이 0보다 큰 것만 선택합니다. (-9999 제외)\n",
    "2. GMI와 DPR의 위/경도 차이를 계산합니다.\n",
    "3. Dataset에서 StandardScale을 하기 위해 mean_vector와 std_vector를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_feather('../input/train.ftr')\n",
    "train_df = train_df.loc[train_df['precipitation']>=0].reset_index(drop=True)\n",
    "\n",
    "train_df['long_GMI'] = train_df['long_GMI'] - train_df['long_DPR']\n",
    "train_df['lat_GMI'] = train_df['lat_GMI'] - train_df['lat_DPR']\n",
    "\n",
    "train_columns = [c for c in train_df.columns if c not in ['precipitation', 'orbit', 'subset', 'pixel']]\n",
    "mean_vector = []\n",
    "std_vector = []\n",
    "for c in train_columns:\n",
    "    train_df[c] = train_df[c].astype(np.float32)\n",
    "    _m = train_df[c].mean()\n",
    "    mean_vector.append(_m)\n",
    "    _s = train_df[c].std()\n",
    "    std_vector.append(_s)\n",
    "mean_vector = np.array(mean_vector)\n",
    "std_vector = np.array(std_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset에 전달할 train file list와 valid file list를 만드는 코드입니다.\n",
    "1. 기존에 저장된 file이 있다면 pickle library를 사용하여 load하고 없다면 새로 만듭니다.\n",
    "2. train path를 정의합니다. train_path = '../input/train'\n",
    "2. train_test_split을 활용하여 validset size는 전체 train에 10%로 할당합니다.\n",
    "3. train 데이터는 target값에 0보다 작은 값이 하나라도 있으면 사용하지 않습니다.\n",
    "4. 외부데이터 폴더를 정의합니다. '../input/newtrain'\n",
    "6. 외부데이터도 마찬가지로 -9999는 사용하지 않고 추가적으로 GMI와 DPR의 위/경도 차가 너무 크면 사용하지 않습니다.\n",
    "7. ValidSet도 마찬가지로 -9999를 사용하지 않고 위/경도 차가 크면 사용하지 않습니다.\n",
    "8. 추후 이 파일을 재사용하기 위하여 위에서 지정한 pickle 파일 이름으로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_files_rmna_new_train2 valid_files_rmna_new_train2\n",
      "76702\n",
      "7585\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "TRAIN_FILE = 'train_files_rmna_new_train2'\n",
    "VALID_FILE = 'valid_' + '_'.join(TRAIN_FILE.split('_')[1:])\n",
    "print(TRAIN_FILE, VALID_FILE)\n",
    "if os.path.exists(TRAIN_FILE):\n",
    "    train_files_rmna=pickle.load(open(TRAIN_FILE,'rb'))\n",
    "    valid_files_rmna=pickle.load(open(VALID_FILE,'rb'))\n",
    "else:\n",
    "    print(\"Make file\")\n",
    "    train_path = '../input/train'\n",
    "    train_files, valid_files = train_test_split(sorted(glob.glob(train_path + '/*')),test_size=0.1,random_state=31014)\n",
    "    train_files = [file.replace('\\\\','/') for file in train_files]\n",
    "    valid_files = [file.replace('\\\\','/') for file in valid_files]\n",
    "    train_files_rmna = []\n",
    "    for file in tqdm(train_files):\n",
    "        dataset = np.load(file)\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        if np.sum(target <0) == 0:\n",
    "            train_files_rmna.append(file)\n",
    "    \n",
    "    new_train_path = '../input/newtrain'\n",
    "    new_train_files = sorted(glob.glob(new_train_path + '/*'))\n",
    "    new_train_files = [file.replace('\\\\','/') for file in new_train_files]\n",
    "    new_train_add_files = []\n",
    "    for file in tqdm(new_train_files):\n",
    "        dataset = np.load(file)\n",
    "        abs_sum = np.sum(np.abs(dataset[:,:,10] - dataset[:,:,12]) + np.abs(dataset[:,:,11] - dataset[:,:,13]))\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        if np.sum(target <0) == 0 and abs_sum < 100:\n",
    "            train_files_rmna.append(file)\n",
    "            new_train_add_files.append(file)\n",
    "    \n",
    "    print(\"Add NewTrain: \", len(new_train_add_files))\n",
    "    \n",
    "    valid_files_rmna = []\n",
    "    for file in tqdm(valid_files):\n",
    "        dataset = np.load(file)\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        abs_sum = np.sum(np.abs(dataset[:,:,10] - dataset[:,:,12]) + np.abs(dataset[:,:,11] - dataset[:,:,13]))\n",
    "        if np.sum(target <0) == 0 and abs_sum < 100:\n",
    "            valid_files_rmna.append(file)\n",
    "            \n",
    "    pickle.dump(train_files_rmna,open(TRAIN_FILE,'wb'))\n",
    "    pickle.dump(valid_files_rmna,open(VALID_FILE,'wb'))\n",
    "    \n",
    "print(len(train_files_rmna))\n",
    "print(len(valid_files_rmna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    ">* Train: 데이터의 50%를 Augmentation 수행, BatchSize 128\n",
    "* Valid: Augmentation 수행 안함, BatchSize 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76702\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files_rmna))\n",
    "train_dataset = Dataset(\n",
    "    train_files_rmna,\n",
    "    is_test_or_not = False,\n",
    "    is_transform=True,\n",
    "    aug_ratio=0.5\n",
    ")\n",
    "\n",
    "valid_dataset = Dataset(\n",
    "    valid_files_rmna,\n",
    "    is_test_or_not=False\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm2d(12)\n",
    "        self.conv0 =  nn.Conv2d(12, 64, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn_128 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv1_1 =  nn.Conv2d(64, 32, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv1_2 =  nn.Conv2d(64, 32, kernel_size=3, stride=1, bias=False, padding=1)\n",
    "        self.conv1_3 =  nn.Conv2d(64, 32, kernel_size=5, stride=1, bias=False, padding=2)\n",
    "\n",
    "        self.conv2_1 =  nn.Conv2d(64, 32, kernel_size=7, stride=1, bias=False, padding=3)\n",
    "        self.conv2_2 =  nn.Conv2d(64, 32, kernel_size=9, stride=1, bias=False, padding=4)\n",
    "        self.conv2_3 =  nn.Conv2d(64, 32, kernel_size=11, stride=1, bias=False, padding=5)\n",
    "\n",
    "        self.bn_concat = nn.BatchNorm2d(64+(32+32+32)*2)\n",
    "\n",
    "        self.bottle_1 = nn.Conv2d(64+(32+32+32)*2,128,kernel_size=1, stride=1, bias=False)\n",
    "        self.bottle_2 = nn.Conv2d(128,64, kernel_size=3, stride=1, bias=False, padding=1)\n",
    "\n",
    "        self.bottle_3 = nn.Conv2d(64+(32+32+32)*2,128,kernel_size=3, stride=1, bias=False, padding=1)\n",
    "        self.bottle_4 = nn.Conv2d(128, 64, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.bn_bottleneck24 = nn.BatchNorm2d(64+(32+32+32)*2+64+64)\n",
    "\n",
    "        self.bottle_5 = nn.Conv2d(64+(32+32+32)*2+64+64,256,kernel_size=1, stride=1, bias=False)\n",
    "        self.bottle_6 = nn.Conv2d(256,128,kernel_size=3, stride=1, bias=False, padding=1)\n",
    "\n",
    "        self.bn_output = nn.BatchNorm2d(128)\n",
    "        self.conv_out = nn.Conv2d(128, 1, kernel_size=1, stride=1, bias=False)\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.bn0(inputs)\n",
    "        conv0 = self.conv0(x)\n",
    "        conv0 = self.bn_128(conv0)\n",
    "        conv0 = self.relu(conv0)\n",
    "\n",
    "        conv1_1 = self.conv1_1(conv0)\n",
    "        conv1_2 = self.conv1_2(conv0)\n",
    "        conv1_3 = self.conv1_3(conv0)\n",
    "\n",
    "        conv2_1 = self.conv2_1(conv0)\n",
    "        conv2_2 = self.conv2_2(conv0)\n",
    "        conv2_3 = self.conv2_3(conv0)\n",
    "\n",
    "        concat = torch.cat([conv0, \n",
    "                            conv1_1, conv1_2, conv1_3, \n",
    "                            conv2_1, conv2_2, conv2_3\n",
    "                            ],axis=1)\n",
    "\n",
    "        concat = self.relu(concat)\n",
    "        bn_concat = self.bn_concat(concat)\n",
    "\n",
    "        bottle_1 = self.bottle_1(bn_concat)\n",
    "        bottle_1 = self.relu(bottle_1)\n",
    "        bottle_2 = self.bottle_2(bottle_1)\n",
    "        bottle_2 = self.relu(bottle_2)\n",
    "\n",
    "        bottle_3 = self.bottle_3(bn_concat)\n",
    "        bottle_3 = self.relu(bottle_3)\n",
    "        bottle_4 = self.bottle_4(bottle_3)\n",
    "        bottle_4 = self.relu(bottle_4)\n",
    "\n",
    "        bottle24_concat = torch.cat([bn_concat, bottle_2, bottle_4],axis=1)\n",
    "        bottle24_concat = self.bn_bottleneck24(bottle24_concat)\n",
    "\n",
    "        bottle_5 = self.bottle_5(bottle24_concat)\n",
    "        bottle_5 = self.relu(bottle_5)\n",
    "        bottle_6 = self.bottle_6(bottle_5)\n",
    "        bottle_6 = self.relu(bottle_6)\n",
    "\n",
    "        conv_out = self.bn_output(bottle_6)\n",
    "        out = self.conv_out(conv_out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out \n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.training:\n",
    "            print('get eval')\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model, Loss, Metrics, optimizer, scheduler 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConv()\n",
    "\n",
    "# MOFLoss란 mae_over_fscore의 줄인말로 계산 방식은 동일합니다.\n",
    "# pytorch로 구현되었습니다.\n",
    "loss = smp.utils.losses.MOFLoss()\n",
    "\n",
    "# 마찬가지로 mae_over_fscore의 pytorch metric version입니다.\n",
    "metrics = [ smp.utils.metrics.MAEOVERFSCORE()]\n",
    "\n",
    "base_optimizer = Ralamb(model.parameters(), weight_decay=0.01)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6) # 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습을 도와주는 Epoch Module에 Model, loss, metrics, optimizer를 넣고 epoch 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 150 epoch까지 학습 수행\n",
    "얼리스탑핑 없이 150epoch까지 수행 후 가장 점수가 좋은 저장된 Model을 불러옵니다.<br>\n",
    "2080ti 한 대에서 약 10~12시간 정도 소요됩니다. <br>\n",
    "Ensemble시 사용한 가장 좋은 Model이름은 아래와 같습니다.<br>\n",
    "code/best_model/NewTrain8330_CustomModel_YM_mof_loss_1.3669144273527496_epoch_130.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "\r",
      "train:   0%|                                                                                   | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████| 600/600 [04:17<00:00,  2.33it/s, mof_loss - 2.128, maeoverfscore - 2.128]\n",
      "Valid All MOF\n",
      "F-Score:  0.7471034805644287\n",
      "MAE:  1.4267327\n",
      "1.9096852116034886\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train:   0%|                                                                                   | 0/600 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimym\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SimpleConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  12%|████▏                            | 75/600 [00:33<03:51,  2.27it/s, mof_loss - 1.972, maeoverfscore - 1.972]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b4bea6836939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nEpoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mof_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Kaggle\\dacon_sate_submission\\code\\segmentation_models_pytorch\\utils\\train.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m# update loss logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Kaggle\\dacon_sate_submission\\code\\segmentation_models_pytorch\\utils\\train.py\u001b[0m in \u001b[0;36mbatch_update\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-502c4a6dacfb>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# print(self.k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#assert id(self.param_groups) == id(self.base_optimizer.param_groups)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lookahead_step'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-502c4a6dacfb>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0mradam_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mradam_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[0mweight_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mweight_norm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mradam_norm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m                     \u001b[0mtrust_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 150\n",
    "\n",
    "min_score = np.Inf\n",
    "MODEL = \"CustomModel_YM\"\n",
    "EXP = \"NewTrain8330\"\n",
    "\n",
    "for i in range(0, NUM_EPOCH):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    mof = get_mof_valid(model, valid_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if min_score > mof:\n",
    "        min_score = mof\n",
    "        torch.save(model, f'best_model/{EXP}_{MODEL}_{loss.__name__}_{mof}_epoch_{i}.pth')\n",
    "        torch.save(model, f'best_model/{EXP}_{MODEL}_{loss.__name__}.pth')\n",
    "        print('Model saved!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. test file의 경로를 지정하고 Dataset class에 test file list를 넘겨서 객체를 생성합니다.\n",
    "2. DataLoader에 batch_size 1로 하여 test dataloader를 만듭니다.\n",
    "3. savedmodel/NewTrain8330_CustomModel_YM_mof_loss.pth을 Load합니다. 재현 시 이 파일이 아닌 다른 파일의 점수가 제일 좋다면 그것으로 사용하여도 큰 차이가 발생하지는 않을 것입니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../input/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*'))\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    test_files, True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model = torch.load('best_model_2_0.12573673413876285.pth') LB 2.8759346226\n",
    "best_model = torch.load('savedmodel/NewTrain8330_CustomModel_YM_mof_loss.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4960d4c610a4f57a0292b9ec2c6ea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_model.eval()\n",
    "results = []\n",
    "for batch_idx, (feature, target) in tqdm(enumerate(test_dataloader)):\n",
    "    results.append(best_model.predict(feature.cuda()).view(-1,1600).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0이하는 0으로 Clip하고 Model3 제출 파일 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(results,axis=0)\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission.iloc[:,1:] =np.clip(preds,0,np.inf)\n",
    "submission.to_csv('../output/NewTrain8330_CustomModel_YM_mof_loss_1.3669144273527496_epoch_130.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
