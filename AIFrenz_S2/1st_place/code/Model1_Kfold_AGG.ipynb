{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Dacon] AI프렌즈 시즌2 강수량 산출 경진대회\n",
    "## giba.kim (팀명)\n",
    "## 2020년 5월 29일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1_Kfold_AGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN 기반 Custom Model KFold\n",
    "\n",
    ">* Data: Original Data, -9999 제거 \n",
    "* Cross Validation: 5 Fold\n",
    "* Loss: MOFLoss\n",
    "* Optimizer: RAdam + LARS + LookAHead (https://github.com/mgrankin/over9000)\n",
    "* Scheduler: CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6)\n",
    "* Model: CNN 기반 Custom Model\n",
    "* Batch: 128\n",
    "* Epoch: 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler,AdamW\n",
    "from torchvision import transforms,models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "RAdam + LARS + LookAHead\n",
    "\n",
    "Lookahead implementation from https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n",
    "RAdam + LARS implementation from https://gist.github.com/redknightlois/c4023d393eb8f92bb44b2ab582d7ec20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        # manually add our defaults to the param groups\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if 'slow_buffer' not in param_state:\n",
    "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
    "                param_state['slow_buffer'].copy_(fast_p.data)\n",
    "            slow = param_state['slow_buffer']\n",
    "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # print(self.k)\n",
    "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group['lookahead_step'] += 1\n",
    "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict['state']\n",
    "        param_groups = fast_state_dict['param_groups']\n",
    "        return {\n",
    "            'state': fast_state,\n",
    "            'slow_state': slow_state,\n",
    "            'param_groups': param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            'state': state_dict['state'],\n",
    "            'param_groups': state_dict['param_groups'],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "\n",
    "        # We want to restore the slow state, but share param_groups reference\n",
    "        # with base_optimizer. This is a bit redundant but least code\n",
    "        slow_state_new = False\n",
    "        if 'slow_state' not in state_dict:\n",
    "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
    "            state_dict['slow_state'] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
    "        if slow_state_new:\n",
    "            # reapply defaults to catch missing lookahead specific ones\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)\n",
    "                    \n",
    "class Ralamb(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(Ralamb, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ralamb, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = radam_step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                radam_step = p_data_fp32.clone()\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
    "\n",
    "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
    "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
    "                if weight_norm == 0 or radam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / radam_norm\n",
    "\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = radam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "\n",
    "                if N_sma >= 5:\n",
    "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대회 Metric Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    _fs = fscore(y_true, y_pred)\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    print(\"F-Score: \", _fs)\n",
    "    print(\"MAE: \", _mae)\n",
    "    return _mae / (_fs + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value fix\n",
    "# seed 값을 고정해야 hyper parameter 바꿀 때마다 결과를 비교할 수 있습니다.\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 0\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, train_files, is_test_or_not=False, is_transform=False, aug_ratio=0):\n",
    "        \"\"\"\n",
    "        train_files: train file list\n",
    "        is_test_or_not: test or not\n",
    "        is_transform: True augmentation\n",
    "        aug_ratio: augmentation ratio\n",
    "        \"\"\"\n",
    "        self.train_files = train_files\n",
    "        self.is_test_or_not=is_test_or_not\n",
    "        self.is_transform = is_transform\n",
    "        self.aug_ratio = aug_ratio\n",
    "    \n",
    "    # data augmenation 6개\n",
    "    # rot90, rot180, rot270, vflip, hflip, transpose\n",
    "    def aug_flip(self, feature, target):\n",
    "        switch = np.random.choice(6) # random 선택\n",
    "        if switch==0: # rot90\n",
    "            feature_aug=np.rot90(feature,k=1,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=1,axes=[1,2]).copy()  \n",
    "        elif switch==1: # rot180\n",
    "            feature_aug=np.rot90(feature,k=2,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=2,axes=[1,2]).copy()\n",
    "        elif switch==2:  # rot270\n",
    "            feature_aug=np.rot90(feature,k=3,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=3,axes=[1,2]).copy()\n",
    "        elif switch==3:  # vflip\n",
    "            feature_aug=np.flip(feature,axis=[1]).copy()\n",
    "            target_aug=np.flip(target,axis=[1]).copy()\n",
    "        elif switch==4:  # hflip\n",
    "            feature_aug=np.flip(feature,axis=[2]).copy()\n",
    "            target_aug=np.flip(target,axis=[2]).copy()\n",
    "        elif switch==5:  # transpose\n",
    "            feature_aug=np.transpose(feature,[0,2,1]).copy()\n",
    "            target_aug=np.transpose(target,[0,2,1]).copy()\n",
    "\n",
    "        return feature_aug, target_aug\n",
    "\n",
    "    def augmetation(self, feature, target):\n",
    "        # uniform한 확률분포를 가져와서 지정된 aug_ratio보다 작으면 augmentation을 하지 않습니다.\n",
    "        aug_prop = np.random.uniform()\n",
    "        is_aug = aug_prop <= self.aug_ratio\n",
    "        if not is_aug:\n",
    "            return feature, target\n",
    "        \n",
    "        feature_aug, target_aug = self.aug_flip(feature, target)\n",
    "            \n",
    "        return feature_aug, target_aug\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        dataset = np.load(self.train_files[i]) \n",
    "        \n",
    "        # target 값 정의\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        \n",
    "        # train일 때 target값은 제외하고 전처리 수행하기 위해서\n",
    "        if not self.is_test_or_not:\n",
    "            dataset = dataset[:,:,:-1]\n",
    "        \n",
    "        # GMI, DPR의 위/경도 diff\n",
    "        dataset[:,:,10] = dataset[:,:,10] - dataset[:,:,12]\n",
    "        dataset[:,:,11] = dataset[:,:,11] - dataset[:,:,13]\n",
    "        \n",
    "        # StandardScaling\n",
    "        norm_temp = (dataset[:,:,:12]-mean_vector[None,None,:12])/std_vector[None,None,:12]\n",
    "        feature = np.moveaxis(norm_temp,-1,0).astype(np.float32)\n",
    "        \n",
    "        # test와 augmentation을 하지 않을 경우 그대로 image와 target return\n",
    "        if self.is_test_or_not or not self.is_transform:\n",
    "            return feature, target\n",
    "        \n",
    "        # augmentation 수행하고 return\n",
    "        return self.augmetation(feature, target)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validset batch size를 2로하면 mae_over_fscore가 계산이 잘 안되는 문제가 있어 validset 전체에 대하여 Metric을 계산하는 Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mof_valid(nn_model, valid_loader):\n",
    "    print(\"Valid All MOF\")\n",
    "    nn_model.eval()\n",
    "    val_results = []\n",
    "    val_targets = []\n",
    "    for batch_idx, (feature, target) in enumerate(valid_loader):\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        target_copy = copy.deepcopy(target)\n",
    "        val_results.append(nn_model.predict(feature_copy.cuda()).view(-1,1600).cpu().numpy())\n",
    "        val_targets.append(target_copy.view(-1,1600).cpu().numpy())\n",
    "        del feature\n",
    "        del target\n",
    "    mof = maeOverFscore(np.concatenate(val_targets),np.concatenate(val_results))\n",
    "    print(mof)\n",
    "    return mof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. train feature파일을 load하여 강수량이 0보다 큰 것만 선택합니다. (-9999 제외)\n",
    "2. GMI와 DPR의 위/경도 차이를 계산합니다.\n",
    "3. Dataset에서 StandardScale을 하기 위해 mean_vector와 std_vector를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cca9069ec3404481a73e8539237ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_feather('../input/train.ftr')\n",
    "train_df = train_df.loc[train_df['precipitation']>=0].reset_index(drop=True)\n",
    "\n",
    "train_df['long_GMI'] = train_df['long_GMI'] - train_df['long_DPR']\n",
    "train_df['lat_GMI'] = train_df['lat_GMI'] - train_df['lat_DPR']\n",
    "\n",
    "train_columns = [c for c in train_df.columns if c not in ['precipitation', 'orbit', 'subset', 'pixel']]\n",
    "mean_vector = []\n",
    "std_vector = []\n",
    "for c in tqdm(train_columns):\n",
    "    train_df[c] = train_df[c].astype(np.float32)\n",
    "    _m = train_df[c].mean()\n",
    "    mean_vector.append(_m)\n",
    "    _s = train_df[c].std()\n",
    "    std_vector.append(_s)\n",
    "mean_vector = np.array(mean_vector)\n",
    "std_vector = np.array(std_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm2d(12)\n",
    "        self.conv0 =  nn.Conv2d(12, 64, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn_128 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv1_1 =  nn.Conv2d(64, 32, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv1_2 =  nn.Conv2d(64, 32, kernel_size=3, stride=1, bias=False, padding=1)\n",
    "        self.conv1_3 =  nn.Conv2d(64, 32, kernel_size=5, stride=1, bias=False, padding=2)\n",
    "\n",
    "        self.conv2_1 =  nn.Conv2d(64, 32, kernel_size=7, stride=1, bias=False, padding=3)\n",
    "        self.conv2_2 =  nn.Conv2d(64, 32, kernel_size=9, stride=1, bias=False, padding=4)\n",
    "        self.conv2_3 =  nn.Conv2d(64, 32, kernel_size=11, stride=1, bias=False, padding=5)\n",
    "\n",
    "        self.bn_concat = nn.BatchNorm2d(64+(32+32+32)*2)\n",
    "\n",
    "        self.bottle_1 = nn.Conv2d(64+(32+32+32)*2,128,kernel_size=1, stride=1, bias=False)\n",
    "        self.bottle_2 = nn.Conv2d(128,64, kernel_size=3, stride=1, bias=False, padding=1)\n",
    "\n",
    "        self.bottle_3 = nn.Conv2d(64+(32+32+32)*2,128,kernel_size=3, stride=1, bias=False, padding=1)\n",
    "        self.bottle_4 = nn.Conv2d(128, 64, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.bn_bottleneck24 = nn.BatchNorm2d(64+(32+32+32)*2+64+64)\n",
    "\n",
    "        self.bottle_5 = nn.Conv2d(64+(32+32+32)*2+64+64,256,kernel_size=1, stride=1, bias=False)\n",
    "        self.bottle_6 = nn.Conv2d(256,128,kernel_size=3, stride=1, bias=False, padding=1)\n",
    "\n",
    "        self.bn_output = nn.BatchNorm2d(128)\n",
    "        self.conv_out = nn.Conv2d(128, 1, kernel_size=1, stride=1, bias=False)\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.bn0(inputs)\n",
    "        conv0 = self.conv0(x)\n",
    "        conv0 = self.bn_128(conv0)\n",
    "        conv0 = self.relu(conv0)\n",
    "\n",
    "        conv1_1 = self.conv1_1(conv0)\n",
    "        conv1_2 = self.conv1_2(conv0)\n",
    "        conv1_3 = self.conv1_3(conv0)\n",
    "\n",
    "        conv2_1 = self.conv2_1(conv0)\n",
    "        conv2_2 = self.conv2_2(conv0)\n",
    "        conv2_3 = self.conv2_3(conv0)\n",
    "\n",
    "        concat = torch.cat([conv0, \n",
    "                            conv1_1, conv1_2, conv1_3, \n",
    "                            conv2_1, conv2_2, conv2_3\n",
    "                            ],axis=1)\n",
    "\n",
    "        concat = self.relu(concat)\n",
    "        bn_concat = self.bn_concat(concat)\n",
    "\n",
    "        bottle_1 = self.bottle_1(bn_concat)\n",
    "        bottle_1 = self.relu(bottle_1)\n",
    "        bottle_2 = self.bottle_2(bottle_1)\n",
    "        bottle_2 = self.relu(bottle_2)\n",
    "\n",
    "        bottle_3 = self.bottle_3(bn_concat)\n",
    "        bottle_3 = self.relu(bottle_3)\n",
    "        bottle_4 = self.bottle_4(bottle_3)\n",
    "        bottle_4 = self.relu(bottle_4)\n",
    "\n",
    "        bottle24_concat = torch.cat([bn_concat, bottle_2, bottle_4],axis=1)\n",
    "        bottle24_concat = self.bn_bottleneck24(bottle24_concat)\n",
    "\n",
    "        bottle_5 = self.bottle_5(bottle24_concat)\n",
    "        bottle_5 = self.relu(bottle_5)\n",
    "        bottle_6 = self.bottle_6(bottle_5)\n",
    "        bottle_6 = self.relu(bottle_6)\n",
    "\n",
    "        conv_out = self.bn_output(bottle_6)\n",
    "        out = self.conv_out(conv_out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out \n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.training:\n",
    "            print('get eval')\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model, Loss, Metrics, optimizer, scheduler 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConv()\n",
    "\n",
    "# MOFLoss란 mae_over_fscore의 줄인말로 계산 방식은 동일합니다.\n",
    "# pytorch로 구현되었습니다.\n",
    "loss = smp.utils.losses.MOFLoss()\n",
    "\n",
    "# 마찬가지로 mae_over_fscore의 pytorch metric version입니다.\n",
    "metrics = [ smp.utils.metrics.MAEOVERFSCORE()]\n",
    "\n",
    "base_optimizer = Ralamb(model.parameters(), weight_decay=0.01)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6) # 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 결과 및 결언\n",
    "## Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. test file의 경로를 지정하고 Dataset class에 test file list를 넘겨서 객체를 생성합니다.\n",
    "2. DataLoader에 batch_size 1로 하여 test dataloader를 만듭니다.\n",
    "3. 각 KFold마다 Best Model을 Load합니다. 재현 시 이 파일이 아닌 다른 파일의 점수가 제일 좋다면 그것으로 사용하여도 큰 차이가 발생하지는 않을 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '../input/test'\n",
    "test_files = sorted(glob.glob(test_path + '/*'))\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    test_files, True\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FOLD Predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimym\\anaconda3\\lib\\site-packages\\torch\\serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "C:\\Users\\kimym\\anaconda3\\lib\\site-packages\\torch\\serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeab20b945e4318a73ffd3e4ab0546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fe68be236e4104a0165b5773fa3f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8a03ce59c44eb383eb41858f147134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322be6fe58db4f9282ce438c5a592b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 FOLD Predict\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91aa050fe7f94b4f80907e10c054cb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch_kfold_model ={1:'1_SimpleConv_mof_loss.pth',\n",
    "                    2:'2_SimpleConv_mof_loss.pth',\n",
    "                    3:'3_SimpleConv_mof_loss.pth',\n",
    "                    4:'4_SimpleConv_mof_loss.pth',\n",
    "                    5:'5_SimpleConv_mof_loss.pth'}\n",
    "preds_all = []\n",
    "for i in range(1,6):\n",
    "    print(f\"{i} FOLD Predict\")\n",
    "    model_name = torch_kfold_model[i]\n",
    "    best_model = torch.load(f'savedmodel/{model_name}')\n",
    "    best_model.eval()\n",
    "    results = []\n",
    "    for batch_idx, (feature, target) in tqdm(enumerate(test_dataloader)):\n",
    "        results.append(best_model.predict(feature.cuda()).view(-1,1600).cpu().numpy())\n",
    "    preds = np.concatenate(results,axis=0)\n",
    "    preds_all.append(preds.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = np.mean(preds_all,axis=0)\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "submission.iloc[:,1:] =np.clip(results_all,0,np.inf)\n",
    "submission.to_csv('../output/ensemble_SimpleConv_mof_loss_5folds_150ep.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
