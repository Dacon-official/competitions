{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Dacon] AI프렌즈 시즌2 강수량 산출 경진대회\n",
    "## giba.kim (팀명)\n",
    "## 2020년 5월 29일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1_Kfold5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN 기반 Custom Model KFold\n",
    "\n",
    ">* Data: Original Data, -9999 제거 \n",
    "* Cross Validation: 5 Fold\n",
    "* Loss: MOFLoss\n",
    "* Optimizer: RAdam + LARS + LookAHead (https://github.com/mgrankin/over9000)\n",
    "* Scheduler: CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6)\n",
    "* Model: CNN 기반 Custom Model\n",
    "* Batch: 128\n",
    "* Epoch: 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import Adam,lr_scheduler,AdamW\n",
    "from torchvision import transforms,models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pretrainedmodels\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "import copy\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Function 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "RAdam + LARS + LookAHead\n",
    "\n",
    "Lookahead implementation from https://github.com/lonePatient/lookahead_pytorch/blob/master/optimizer.py\n",
    "RAdam + LARS implementation from https://gist.github.com/redknightlois/c4023d393eb8f92bb44b2ab582d7ec20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lookahead(Optimizer):\n",
    "    def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "        if not 0.0 <= alpha <= 1.0:\n",
    "            raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "        if not 1 <= k:\n",
    "            raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "        self.base_optimizer = base_optimizer\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults = base_optimizer.defaults\n",
    "        self.defaults.update(defaults)\n",
    "        self.state = defaultdict(dict)\n",
    "        # manually add our defaults to the param groups\n",
    "        for name, default in defaults.items():\n",
    "            for group in self.param_groups:\n",
    "                group.setdefault(name, default)\n",
    "\n",
    "    def update_slow(self, group):\n",
    "        for fast_p in group[\"params\"]:\n",
    "            if fast_p.grad is None:\n",
    "                continue\n",
    "            param_state = self.state[fast_p]\n",
    "            if 'slow_buffer' not in param_state:\n",
    "                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
    "                param_state['slow_buffer'].copy_(fast_p.data)\n",
    "            slow = param_state['slow_buffer']\n",
    "            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
    "            fast_p.data.copy_(slow)\n",
    "\n",
    "    def sync_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update_slow(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        # print(self.k)\n",
    "        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n",
    "        loss = self.base_optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group['lookahead_step'] += 1\n",
    "            if group['lookahead_step'] % group['lookahead_k'] == 0:\n",
    "                self.update_slow(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.base_optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict['state']\n",
    "        param_groups = fast_state_dict['param_groups']\n",
    "        return {\n",
    "            'state': fast_state,\n",
    "            'slow_state': slow_state,\n",
    "            'param_groups': param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            'state': state_dict['state'],\n",
    "            'param_groups': state_dict['param_groups'],\n",
    "        }\n",
    "        self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "\n",
    "        # We want to restore the slow state, but share param_groups reference\n",
    "        # with base_optimizer. This is a bit redundant but least code\n",
    "        slow_state_new = False\n",
    "        if 'slow_state' not in state_dict:\n",
    "            print('Loading state_dict from optimizer without Lookahead applied.')\n",
    "            state_dict['slow_state'] = defaultdict(dict)\n",
    "            slow_state_new = True\n",
    "        slow_state_dict = {\n",
    "            'state': state_dict['slow_state'],\n",
    "            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
    "        }\n",
    "        super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
    "        if slow_state_new:\n",
    "            # reapply defaults to catch missing lookahead specific ones\n",
    "            for name, default in self.defaults.items():\n",
    "                for group in self.param_groups:\n",
    "                    group.setdefault(name, default)\n",
    "                    \n",
    "class Ralamb(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(Ralamb, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ralamb, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Ralamb does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, radam_step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = radam_step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                radam_step = p_data_fp32.clone()\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
    "\n",
    "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
    "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
    "                if weight_norm == 0 or radam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / radam_norm\n",
    "\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = radam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "\n",
    "                if N_sma >= 5:\n",
    "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대회 Metric Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def mae(y_true, y_pred) :\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    over_threshold = y_true >= 0.1\n",
    "    \n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):\n",
    "    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    \n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    \n",
    "    remove_NAs = y_true >= 0\n",
    "    \n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    _fs = fscore(y_true, y_pred)\n",
    "    _mae = mae(y_true, y_pred)\n",
    "    print(\"F-Score: \", _fs)\n",
    "    print(\"MAE: \", _mae)\n",
    "    return _mae / (_fs + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed value fix\n",
    "# seed 값을 고정해야 hyper parameter 바꿀 때마다 결과를 비교할 수 있습니다.\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 0\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, train_files, is_test_or_not=False, is_transform=False, aug_ratio=0):\n",
    "        \"\"\"\n",
    "        train_files: train file list\n",
    "        is_test_or_not: test or not\n",
    "        is_transform: True augmentation\n",
    "        aug_ratio: augmentation ratio\n",
    "        \"\"\"\n",
    "        self.train_files = train_files\n",
    "        self.is_test_or_not=is_test_or_not\n",
    "        self.is_transform = is_transform\n",
    "        self.aug_ratio = aug_ratio\n",
    "    \n",
    "    # data augmenation 6개\n",
    "    # rot90, rot180, rot270, vflip, hflip, transpose\n",
    "    def aug_flip(self, feature, target):\n",
    "        switch = np.random.choice(6) # random 선택\n",
    "        if switch==0: # rot90\n",
    "            feature_aug=np.rot90(feature,k=1,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=1,axes=[1,2]).copy()  \n",
    "        elif switch==1: # rot180\n",
    "            feature_aug=np.rot90(feature,k=2,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=2,axes=[1,2]).copy()\n",
    "        elif switch==2:  # rot270\n",
    "            feature_aug=np.rot90(feature,k=3,axes=[1,2]).copy()\n",
    "            target_aug=np.rot90(target,k=3,axes=[1,2]).copy()\n",
    "        elif switch==3:  # vflip\n",
    "            feature_aug=np.flip(feature,axis=[1]).copy()\n",
    "            target_aug=np.flip(target,axis=[1]).copy()\n",
    "        elif switch==4:  # hflip\n",
    "            feature_aug=np.flip(feature,axis=[2]).copy()\n",
    "            target_aug=np.flip(target,axis=[2]).copy()\n",
    "        elif switch==5:  # transpose\n",
    "            feature_aug=np.transpose(feature,[0,2,1]).copy()\n",
    "            target_aug=np.transpose(target,[0,2,1]).copy()\n",
    "\n",
    "        return feature_aug, target_aug\n",
    "\n",
    "    def augmetation(self, feature, target):\n",
    "        # uniform한 확률분포를 가져와서 지정된 aug_ratio보다 작으면 augmentation을 하지 않습니다.\n",
    "        aug_prop = np.random.uniform()\n",
    "        is_aug = aug_prop <= self.aug_ratio\n",
    "        if not is_aug:\n",
    "            return feature, target\n",
    "        \n",
    "        feature_aug, target_aug = self.aug_flip(feature, target)\n",
    "            \n",
    "        return feature_aug, target_aug\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        try:\n",
    "            dataset = np.load(self.train_files[i]) \n",
    "        except:\n",
    "            dataset = np.load(self.train_files[i].replace('\\\\','/')) \n",
    "        \n",
    "        # target 값 정의\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        \n",
    "        # train일 때 target값은 제외하고 전처리 수행하기 위해서\n",
    "        if not self.is_test_or_not:\n",
    "            dataset = dataset[:,:,:-1]\n",
    "        \n",
    "        # GMI, DPR의 위/경도 diff\n",
    "        dataset[:,:,10] = dataset[:,:,10] - dataset[:,:,12]\n",
    "        dataset[:,:,11] = dataset[:,:,11] - dataset[:,:,13]\n",
    "        \n",
    "        # StandardScaling\n",
    "        norm_temp = (dataset[:,:,:12]-mean_vector[None,None,:12])/std_vector[None,None,:12]\n",
    "        feature = np.moveaxis(norm_temp,-1,0).astype(np.float32)\n",
    "        \n",
    "        # test와 augmentation을 하지 않을 경우 그대로 image와 target return\n",
    "        if self.is_test_or_not or not self.is_transform:\n",
    "            return feature, target\n",
    "        \n",
    "        # augmentation 수행하고 return\n",
    "        return self.augmetation(feature, target)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validset batch size를 2로하면 mae_over_fscore가 계산이 잘 안되는 문제가 있어 validset 전체에 대하여 Metric을 계산하는 Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mof_valid(nn_model, valid_loader):\n",
    "    print(\"Valid All MOF\")\n",
    "    nn_model.eval()\n",
    "    val_results = []\n",
    "    val_targets = []\n",
    "    for batch_idx, (feature, target) in enumerate(valid_loader):\n",
    "        feature_copy = copy.deepcopy(feature)\n",
    "        target_copy = copy.deepcopy(target)\n",
    "        val_results.append(nn_model.predict(feature_copy.cuda()).view(-1,1600).cpu().numpy())\n",
    "        val_targets.append(target_copy.view(-1,1600).cpu().numpy())\n",
    "        del feature\n",
    "        del target\n",
    "    mof = maeOverFscore(np.concatenate(val_targets),np.concatenate(val_results))\n",
    "    print(mof)\n",
    "    return mof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리\n",
    "## Data Cleansing & Pre-Processing  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. train feature파일을 load하여 강수량이 0보다 큰 것만 선택합니다. (-9999 제외)\n",
    "2. GMI와 DPR의 위/경도 차이를 계산합니다.\n",
    "3. Dataset에서 StandardScale을 하기 위해 mean_vector와 std_vector를 계산합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d2b326ed1346409e5724db3027ec53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_feather('../input/train.ftr')\n",
    "train_df = train_df.loc[train_df['precipitation']>=0].reset_index(drop=True)\n",
    "\n",
    "train_df['long_GMI'] = train_df['long_GMI'] - train_df['long_DPR']\n",
    "train_df['lat_GMI'] = train_df['lat_GMI'] - train_df['lat_DPR']\n",
    "\n",
    "train_columns = [c for c in train_df.columns if c not in ['precipitation', 'orbit', 'subset', 'pixel']]\n",
    "mean_vector = []\n",
    "std_vector = []\n",
    "for c in train_columns:\n",
    "    train_df[c] = train_df[c].astype(np.float32)\n",
    "    _m = train_df[c].mean()\n",
    "    mean_vector.append(_m)\n",
    "    _s = train_df[c].std()\n",
    "    std_vector.append(_s)\n",
    "mean_vector = np.array(mean_vector)\n",
    "std_vector = np.array(std_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.bn0 = nn.BatchNorm2d(12)\n",
    "        self.conv0 =  nn.Conv2d(12, 64, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn_128 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv1_1 =  nn.Conv2d(64, 32, kernel_size=1, stride=1, bias=False)\n",
    "        self.conv1_2 =  nn.Conv2d(64, 32, kernel_size=3, stride=1, bias=False, padding=1)\n",
    "        self.conv1_3 =  nn.Conv2d(64, 32, kernel_size=5, stride=1, bias=False, padding=2)\n",
    "\n",
    "        self.conv2_1 =  nn.Conv2d(64, 32, kernel_size=7, stride=1, bias=False, padding=3)\n",
    "        self.conv2_2 =  nn.Conv2d(64, 32, kernel_size=9, stride=1, bias=False, padding=4)\n",
    "        self.conv2_3 =  nn.Conv2d(64, 32, kernel_size=11, stride=1, bias=False, padding=5)\n",
    "\n",
    "        self.bn_concat = nn.BatchNorm2d(64+(32+32+32)*2)\n",
    "\n",
    "        self.bottle_1 = nn.Conv2d(64+(32+32+32)*2,128,kernel_size=1, stride=1, bias=False)\n",
    "        self.bottle_2 = nn.Conv2d(128,64, kernel_size=3, stride=1, bias=False, padding=1)\n",
    "\n",
    "        self.bottle_3 = nn.Conv2d(64+(32+32+32)*2,128,kernel_size=3, stride=1, bias=False, padding=1)\n",
    "        self.bottle_4 = nn.Conv2d(128, 64, kernel_size=1, stride=1, bias=False)\n",
    "\n",
    "        self.bn_bottleneck24 = nn.BatchNorm2d(64+(32+32+32)*2+64+64)\n",
    "\n",
    "        self.bottle_5 = nn.Conv2d(64+(32+32+32)*2+64+64,256,kernel_size=1, stride=1, bias=False)\n",
    "        self.bottle_6 = nn.Conv2d(256,128,kernel_size=3, stride=1, bias=False, padding=1)\n",
    "\n",
    "        self.bn_output = nn.BatchNorm2d(128)\n",
    "        self.conv_out = nn.Conv2d(128, 1, kernel_size=1, stride=1, bias=False)\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        x = self.bn0(inputs)\n",
    "        conv0 = self.conv0(x)\n",
    "        conv0 = self.bn_128(conv0)\n",
    "        conv0 = self.relu(conv0)\n",
    "\n",
    "        conv1_1 = self.conv1_1(conv0)\n",
    "        conv1_2 = self.conv1_2(conv0)\n",
    "        conv1_3 = self.conv1_3(conv0)\n",
    "\n",
    "        conv2_1 = self.conv2_1(conv0)\n",
    "        conv2_2 = self.conv2_2(conv0)\n",
    "        conv2_3 = self.conv2_3(conv0)\n",
    "\n",
    "        concat = torch.cat([conv0, \n",
    "                            conv1_1, conv1_2, conv1_3, \n",
    "                            conv2_1, conv2_2, conv2_3\n",
    "                            ],axis=1)\n",
    "\n",
    "        concat = self.relu(concat)\n",
    "        bn_concat = self.bn_concat(concat)\n",
    "\n",
    "        bottle_1 = self.bottle_1(bn_concat)\n",
    "        bottle_1 = self.relu(bottle_1)\n",
    "        bottle_2 = self.bottle_2(bottle_1)\n",
    "        bottle_2 = self.relu(bottle_2)\n",
    "\n",
    "        bottle_3 = self.bottle_3(bn_concat)\n",
    "        bottle_3 = self.relu(bottle_3)\n",
    "        bottle_4 = self.bottle_4(bottle_3)\n",
    "        bottle_4 = self.relu(bottle_4)\n",
    "\n",
    "        bottle24_concat = torch.cat([bn_concat, bottle_2, bottle_4],axis=1)\n",
    "        bottle24_concat = self.bn_bottleneck24(bottle24_concat)\n",
    "\n",
    "        bottle_5 = self.bottle_5(bottle24_concat)\n",
    "        bottle_5 = self.relu(bottle_5)\n",
    "        bottle_6 = self.bottle_6(bottle_5)\n",
    "        bottle_6 = self.relu(bottle_6)\n",
    "\n",
    "        conv_out = self.bn_output(bottle_6)\n",
    "        out = self.conv_out(conv_out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out \n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.training:\n",
    "            print('get eval')\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model, Loss, Metrics, optimizer, scheduler 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleConv()\n",
    "\n",
    "# MOFLoss란 mae_over_fscore의 줄인말로 계산 방식은 동일합니다.\n",
    "# pytorch로 구현되었습니다.\n",
    "loss = smp.utils.losses.MOFLoss()\n",
    "\n",
    "# 마찬가지로 mae_over_fscore의 pytorch metric version입니다.\n",
    "metrics = [ smp.utils.metrics.MAEOVERFSCORE()]\n",
    "\n",
    "base_optimizer = Ralamb(model.parameters(), weight_decay=0.01)\n",
    "optimizer = Lookahead(base_optimizer)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6) # 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습을 도와주는 Epoch Module에 Model, loss, metrics, optimizer를 넣고 epoch 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda'\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset에 전달할 train file list와 valid file list를 만드는 코드입니다.\n",
    "1. 기존에 저장된 file이 있다면 pickle library를 사용하여 load하고 없다면 새로 만듭니다.\n",
    "2. train path를 정의합니다. train_path = '../input/train'\n",
    "2. CrossValidation을 하기 때문에 따로 Validationd을 두지 않습니다.\n",
    "3. train 데이터는 target값에 0보다 작은 값이 하나라도 있으면 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "TRAIN_FILE = 'train_files_rmna_kfold'\n",
    "if os.path.exists(TRAIN_FILE):\n",
    "    train_files_rmna=pickle.load(open(TRAIN_FILE,'rb'))\n",
    "else:\n",
    "    print(\"Make file\")\n",
    "    train_path = '../input/train/'\n",
    "    train_files = sorted(glob.glob(train_path + '/*'))\n",
    "    train_files = [file.replace('\\\\','/') for file in train_files]\n",
    "    \n",
    "    train_files_rmna = []\n",
    "    for file in tqdm(train_files):\n",
    "        dataset = np.load(file)\n",
    "        target= np.moveaxis(dataset[:,:,-1].reshape(40,40,1),-1,0).astype(np.float32)\n",
    "        if np.sum(target <0) == 0:\n",
    "            train_files_rmna.append(file)\n",
    "    \n",
    "    pickle.dump(train_files_rmna,open(TRAIN_FILE,'wb'))\n",
    "    \n",
    "    print(len(train_files_rmna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5 FOLD Start\n",
      "60766 15191\n",
      "\n",
      "Epoch: 0\n",
      "\r",
      "train:   0%|                                                                                   | 0/475 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\torch\\csrc\\utils\\python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 100%|████████████████████████████████| 475/475 [03:08<00:00,  2.51it/s, mof_loss - 2.131, maeoverfscore - 2.131]\n",
      "Valid All MOF\n",
      "F-Score:  0.7331986046487284\n",
      "MAE:  1.3676438\n",
      "1.8653113057743758\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train:   0%|                                                                                   | 0/475 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimym\\anaconda3\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SimpleConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  13%|████▏                            | 60/475 [00:25<02:54,  2.38it/s, mof_loss - 1.985, maeoverfscore - 1.985]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-667f3e5c67c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_EPOCH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nEpoch: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mtrain_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0mmof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mof_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Kaggle\\dacon_sate_submission\\code\\segmentation_models_pytorch\\utils\\train.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1105\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1108\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d44cebd7fea7>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;31m# target 값 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[1;32m--> 453\u001b[1;33m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[0;32m    454\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Try a pickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\u001b[0m in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m             \u001b[1;31m# We can use the fast fromfile() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    756\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;31m# This is not a real file. We have to read it the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# kfold용 array 정의\n",
    "train_files_rmna = np.array(train_files_rmna)\n",
    "\n",
    "# 5fold shuffle true, random seed는 0으로 하는 cross validation 구축\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# aws 5대를 활용하여 각 fold마다 한 machine씩 할당하여 결과 도출하기 위하여 fold index를 지정\n",
    "# 해당 fold index가 아니면 수행하지 않는다.\n",
    "target_fold_index = 5\n",
    "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(train_files_rmna),1):\n",
    "    print(fold_index)\n",
    "    if fold_index == target_fold_index:\n",
    "        trn_file, val_file = train_files_rmna[trn_idx], train_files_rmna[val_idx]\n",
    "        \n",
    "        # model, loss, metric, optimizer, scheduler 선언\n",
    "        model = SimpleConv()\n",
    "        loss = smp.utils.losses.MOFLoss()\n",
    "        metrics = [ smp.utils.metrics.MAEOVERFSCORE()]\n",
    "        base_optimizer = Ralamb(model.parameters(), weight_decay=0.01)\n",
    "        optimizer = Lookahead(base_optimizer)\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, 10, 2,eta_min=1e-6) # 1e-6\n",
    "        \n",
    "        # 학습을 도와주는 Epoch Module에 Model, loss, metrics, optimizer를 넣고 epoch 객체 생성\n",
    "        train_epoch = smp.utils.train.TrainEpoch(model, \n",
    "                                                 loss=loss, \n",
    "                                                 metrics=metrics, \n",
    "                                                 optimizer=optimizer,\n",
    "                                                 device=DEVICE,\n",
    "                                                 verbose=True)\n",
    "\n",
    "        valid_epoch = smp.utils.train.ValidEpoch(model, \n",
    "                                                 loss=loss, \n",
    "                                                 metrics=metrics, \n",
    "                                                 device=DEVICE,\n",
    "                                                 verbose=True)\n",
    "        \n",
    "        # Kfold Dataset 객체 생성\n",
    "        # train은 augmentation 수행하고, validation은 수행하지 않음\n",
    "        print(f\"{fold_index} FOLD Start\")\n",
    "        print(len(trn_file), len(val_file))\n",
    "        train_dataset = Dataset(\n",
    "            trn_file,\n",
    "            is_test_or_not = False,\n",
    "            is_transform=True,\n",
    "            aug_ratio=0.5\n",
    "        )\n",
    "\n",
    "        valid_dataset = Dataset(\n",
    "            val_file,\n",
    "            is_test_or_not=False\n",
    "        )\n",
    "        \n",
    "        # dataloader 생성\n",
    "        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "        # 한 폴드당 150 Epoch 수행\n",
    "        NUM_EPOCH = 150\n",
    "        min_score = np.Inf\n",
    "        MODEL = \"SimpleConv\"\n",
    "        for i in range(0, NUM_EPOCH):\n",
    "            print('\\nEpoch: {}'.format(i))\n",
    "            train_logs = train_epoch.run(train_loader)\n",
    "            mof = get_mof_valid(model, valid_loader)\n",
    "            scheduler.step()\n",
    "            if min_score > mof:\n",
    "                min_score = mof\n",
    "                torch.save(model, f'best_model/{fold_index}_{MODEL}_{loss.__name__}_{mof}_epoch_{i}.pth')\n",
    "                torch.save(model, f'best_model/{fold_index}_{MODEL}_{loss.__name__}.pth')\n",
    "                print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
