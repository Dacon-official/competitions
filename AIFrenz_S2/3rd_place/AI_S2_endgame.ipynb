{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Nj76qUe2lnA"
   },
   "source": [
    "## [Dacon] AI프렌즈 시즌2 강수량 산출 경진대회\n",
    "## 팀: endgame\n",
    "## 2020년 6월 1일 (제출날짜)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[requirement.txt 다운로드 하기](https://drive.google.com//uc?export=download&id=1_aCOQJsMJk2PzLxCUMOgDRnyX7YL98wY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <div style=\"color:red\">README</div>\n",
    "- 외부데이터 및 pretrained 모델을 사용하지 않았습니다.\n",
    "\n",
    "- 하드웨어 리소스가 많이 소요되는 코드입니다. 제 컴퓨터의 램이 128GB라서 모든 데이터를 램에 올려놓고 작업을 했습니다.\n",
    "- 저 같은 경우, EDA, 모델학습을 각각 다른 ipynb 파일에서 작업을 진행했는데, 제출용 파일이다보니 모든 코드를 한 곳에 모아 실행하기에 메모리가 부족할 가능성이 커질 것 같습니다.\n",
    "- train.zip, test.zip 파일은 각각 data/train, data/test 폴더에 압축을 해제해주세요.\n",
    "- sample_submission.csv는 data 폴더에 위치시켜 주세요.\n",
    "- GPU: RTX 2070 super 기준 모델 1: 10시간, 모델2: 10시간, 모델3: 14시간 정도 소요 되었습니다.\n",
    "- 제가 학습시킨 weight를 로드하고 싶으시다면, model1.h5, model2.h5, model3.h5를 ipynb 파일이 있는 곳에 위치시켜 주세요.\n",
    "\n",
    "<div style=\"color:red\">혹시 위의 글을 안 읽으셨다면 꼭 읽어주세요!</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYHb_Mf-2lnG"
   },
   "source": [
    "## 1. 라이브러리 및 데이터 (Library & Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "RtKnyRor2lnI"
   },
   "outputs": [],
   "source": [
    "# 파일관리 및 파일선택\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import gc\n",
    "\n",
    "# 시각화\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import SeparableConv2D, Input, Conv2D, Add, BatchNormalization, concatenate, AveragePooling2D, add, MaxPooling2D, Conv2DTranspose, Activation, Dropout, ZeroPadding2D, LeakyReLU\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "\n",
    "SEED = 30\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred) :\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    over_threshold = y_true >= 0.1\n",
    "\n",
    "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "def fscore(y_true, y_pred):    \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    y_true = y_true.reshape(1, -1)[0]\n",
    "    y_pred = y_pred.reshape(1, -1)[0]\n",
    "    remove_NAs = y_true >= 0\n",
    "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "    return(f1_score(y_true, y_pred))\n",
    "\n",
    "def maeOverFscore(y_true, y_pred):\n",
    "    \n",
    "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zjQY_KY2lnR"
   },
   "source": [
    "## 2. 데이터 전처리 (Data Cleansing & Pre-Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 데이터셋을 만들고 pickle로 저장\n",
    "- -9999와 같은 missing value가 들어있으면 제거\n",
    "- 0.1 이상 내린 픽셀이 UPPER 값 이상인 사진만 데이터셋에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "pb0OD3v82lnT"
   },
   "outputs": [],
   "source": [
    "dir_train = 'data/train/'\n",
    "dir_test = 'data/test/'\n",
    "UPPER = 50\n",
    "\n",
    "def make_dataset(dir_train, dir_test, UPPER):\n",
    "    # train dataset\n",
    "    train = []\n",
    "    train_y = []\n",
    "\n",
    "    for i in os.listdir(dir_train):\n",
    "        npy = np.load(dir_train + i)\n",
    "\n",
    "        # missing value 제거\n",
    "        if npy[:, :, -1].sum() < 0:\n",
    "            continue\n",
    "        \n",
    "        # 0.1이상 내린 픽셀이 UPPER 값 이상인 사진만\n",
    "        if (npy[:, :, -1] >= 0.1).sum() >= UPPER:\n",
    "            train.append(npy[:, :, :-1])\n",
    "            train_y.append(npy[:, :, -1])\n",
    "\n",
    "    train = np.array(train)\n",
    "    train_y = np.array(train_y)\n",
    "\n",
    "    with open(f'data/train{UPPER}.pickle', 'wb') as f:\n",
    "        pickle.dump(train, f, protocol=4)\n",
    "\n",
    "    with open(f'data/train_y{UPPER}.pickle', 'wb') as f:\n",
    "        pickle.dump(train_y, f, protocol=4)\n",
    "\n",
    "    del train\n",
    "    del train_y\n",
    "\n",
    "    # test dataset\n",
    "    test = []\n",
    "\n",
    "    for i in os.listdir(dir_test):\n",
    "        npy = np.load(dir_test + i)\n",
    "        test.append(npy)\n",
    "    test = np.array(test)\n",
    "\n",
    "    with open('data/test.pickle', 'wb') as f:\n",
    "        pickle.dump(test, f, protocol=4)\n",
    "    del test\n",
    "    \n",
    "make_dataset(dir_train, dir_test, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 pickle 파일 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train50.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "# 0~9번채널만 사용\n",
    "train = train[:, :, :, :10]\n",
    "\n",
    "with open('data/train_y50.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "train_y = train_y.reshape(train_y.shape[0], 40, 40, 1)\n",
    "\n",
    "\n",
    "with open('data/test.pickle', 'rb') as f:\n",
    "    TEST = pickle.load(f)\n",
    "TEST = TEST[:, :, :, :10] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwLtCHGC2lnb"
   },
   "source": [
    "## 3. 탐색적 자료분석 (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BliMtn4E2lnd"
   },
   "source": [
    "### 3.1 시각화를 이용한 EDA\n",
    "- v별, h별 합계 피쳐를 만들고 강수량과의 관계를 시각적으로 파악해봤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    ch15_v = 0\n",
    "    for i in [0,2,4,5,7]:\n",
    "        ch15_v += img[:,:,i]\n",
    "    ch15_h = 0\n",
    "    for i in [1,3,6,8]:\n",
    "        ch15_h += img[:,:,i]\n",
    "    ch15_v = ch15_v.reshape(40,40,1)\n",
    "    ch15_h = ch15_h.reshape(40,40,1)\n",
    "    img = np.concatenate([img, ch15_v], -1)\n",
    "    img = np.concatenate([img, ch15_h], -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "image_dir = os.listdir('data/train/')\n",
    "image_sample = np.load(f'data/train/{image_dir[random.randrange(len(image_dir))]}')\n",
    "image_sample = show_img(image_sample)\n",
    "\n",
    "color_map = plt.cm.get_cmap('RdBu')\n",
    "color_map = color_map.reversed()\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(2,6,i+1)\n",
    "    plt.imshow(image_sample[:, :, i], cmap=color_map)\n",
    "    plt.title(f'ch_{i}', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,10)\n",
    "plt.imshow(image_sample[:,:,-3], cmap = color_map)\n",
    "plt.title('rain', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,11)\n",
    "plt.imshow(image_sample[:,:,-2], cmap = color_map)\n",
    "plt.title('v_sum', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplot(2,6,12)\n",
    "plt.imshow(image_sample[:,:,-1], cmap = color_map)\n",
    "plt.title('h_sum', fontdict= {'fontsize': 16})\n",
    "\n",
    "plt.subplots_adjust(top=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 상관관계를 이용한 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = train.reshape(train.shape[0] * train.shape[1] * train.shape[2], train.shape[3])\n",
    "train_y2 = train_y.reshape(train_y.shape[0] * train_y.shape[1] * train_y.shape[2], train_y.shape[3])\n",
    "train_y2 = np.log(train_y2+1)\n",
    "train2 = np.concatenate([train2, train_y2], -1)\n",
    "\n",
    "df_corr = pd.DataFrame(train2).reset_index(drop=True)\n",
    "del train2, train_y2\n",
    "df_corr = df_corr.iloc[400::1600, :]\n",
    "df_corr = df_corr.reset_index(drop=True)\n",
    "df_corr = df_corr.rename(columns={0:'v1',1:'h1',2:'v2',3:'h2',4:'v3',5:'v4',\n",
    "                                  6:'h4',7:'v5',8:'h5',9:'surface',10:'target'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v, h 채널의 합 또는 차이가 도움을 줄 지 확인을 해봤습니다. 아주 조금 상관관계가 상승하는 것을 확인할 수 있었고,  \n",
    "두 피쳐간의 합과 차이는 45도 회전변환 시의 상관관계와 같다는 아이디어에서 착안, 각각 30도, 45, 60도 회전변환해보았습니다.  \n",
    "그 결과 v1-h1, v2-h2, v4-h4는 45도 회전변환시 아주 조금 상관관계가 증가하였고,  \n",
    "v5-h5는 30도 변환 시 상관관계가 매우 크게 증가하는 것을 확인하여 회전변환한 피쳐를 사용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corr['v1_p_h1_30'] = df_corr['v1'] * np.cos(np.pi / 6) + df_corr['h1'] * np.sin(np.pi / 6)\n",
    "# df_corr['v1_m_h1_30'] = df_corr['v1'] * np.cos(np.pi / 6) - df_corr['h1'] * np.sin(np.pi / 6)\n",
    "df_corr['ch1_rot1'] = df_corr['v1'] * np.cos(np.pi / 4) + df_corr['h1'] * np.sin(np.pi / 4)\n",
    "df_corr['ch1_rot2'] = df_corr['v1'] * np.cos(np.pi / 4) - df_corr['h1'] * np.sin(np.pi / 4)\n",
    "# df_corr['v1_p_h1_60'] = df_corr['v1'] * np.cos(np.pi / 3) + df_corr['h1'] * np.sin(np.pi / 3)\n",
    "# df_corr['v1_m_h1_60'] = df_corr['v1'] * np.cos(np.pi / 3) - df_corr['h1'] * np.sin(np.pi / 3)\n",
    "\n",
    "\n",
    "\n",
    "# df_corr['v2_p_h2_30'] = df_corr['v2'] * np.cos(np.pi / 6) + df_corr['h2'] * np.sin(np.pi / 6)\n",
    "# df_corr['v2_m_h2_30'] = df_corr['v2'] * np.cos(np.pi / 6) - df_corr['h2'] * np.sin(np.pi / 6)\n",
    "df_corr['ch2_rot1'] = df_corr['v2'] * np.cos(np.pi / 4) + df_corr['h2'] * np.sin(np.pi / 4)\n",
    "df_corr['ch2_rot2'] = df_corr['v2'] * np.cos(np.pi / 4) - df_corr['h2'] * np.sin(np.pi / 4)\n",
    "# df_corr['v2_p_h2_60'] = df_corr['v2'] * np.cos(np.pi / 3) + df_corr['h2'] * np.sin(np.pi / 3)\n",
    "# df_corr['v2_m_h2_60'] = df_corr['v2'] * np.cos(np.pi / 3) - df_corr['h2'] * np.sin(np.pi / 3)\n",
    "\n",
    "\n",
    "# df_corr['v4_p_h4_30'] = df_corr['v4'] * np.cos(np.pi / 6) + df_corr['h4'] * np.sin(np.pi / 6)\n",
    "# df_corr['v4_m_h4_30'] = df_corr['v4'] * np.cos(np.pi / 6) - df_corr['h4'] * np.sin(np.pi / 6)\n",
    "df_corr['ch4_rot1'] = df_corr['v4'] * np.cos(np.pi / 4) + df_corr['h4'] * np.sin(np.pi / 4)\n",
    "df_corr['ch4_rot2'] = df_corr['v4'] * np.cos(np.pi / 4) - df_corr['h4'] * np.sin(np.pi / 4)\n",
    "# df_corr['v4_p_h4_60'] = df_corr['v4'] * np.cos(np.pi / 3) + df_corr['h4'] * np.sin(np.pi / 3)\n",
    "# df_corr['v4_m_h4_60'] = df_corr['v4'] * np.cos(np.pi / 3) - df_corr['h4'] * np.sin(np.pi / 3)\n",
    "\n",
    "\n",
    "df_corr['ch5_rot1'] = df_corr['v5'] * np.cos(np.pi / 6) + df_corr['h5'] * np.sin(np.pi / 6)\n",
    "df_corr['ch5_rot2'] = df_corr['v5'] * np.cos(np.pi / 6) - df_corr['h5'] * np.sin(np.pi / 6)\n",
    "# df_corr['v5_p_h5_45'] = df_corr['v5'] * np.cos(np.pi / 4) + df_corr['h5'] * np.sin(np.pi / 4)\n",
    "# df_corr['v5_m_h5_45'] = df_corr['v5'] * np.cos(np.pi / 4) - df_corr['h5'] * np.sin(np.pi / 4)\n",
    "# df_corr['v5_p_h5_60'] = df_corr['v5'] * np.cos(np.pi / 3) + df_corr['h5'] * np.sin(np.pi / 3)\n",
    "# df_corr['v5_m_h5_60'] = df_corr['v5'] * np.cos(np.pi / 3) - df_corr['h5'] * np.sin(np.pi / 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr.corr()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_corr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qyq90ZzB2lnk"
   },
   "source": [
    "## 4. 변수 선택 및 모델 구축 (Feature Engineering & Initial Modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 target 값 로그 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.log(train_y+1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 vertical, horizontal 별로 Sum한 피쳐 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rel1AkRP2lnm"
   },
   "outputs": [],
   "source": [
    "def channel_sum(data):\n",
    "    data_v = data[:, :, :, 0].copy() + data[:, :, :, 2].copy() + data[:, :, :, 4].copy() + data[:, :, :, 5].copy() +data[:, :, :, 7].copy()\n",
    "    data_h = data[:, :, :, 1].copy() + data[:, :, :, 3].copy() + data[:, :, :, 6].copy() + data[:, :, :, 8].copy()\n",
    "\n",
    "    data_v = data_v.reshape(data_v.shape[0], data_v.shape[1], data_v.shape[2], 1)\n",
    "    data_h = data_h.reshape(data_h.shape[0], data_h.shape[1], data_h.shape[2], 1)\n",
    "\n",
    "    data = np.concatenate([data, data_v.copy()], -1)\n",
    "    data = np.concatenate([data, data_h.copy()], -1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = channel_sum(train)\n",
    "TEST = channel_sum(TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 9번 채널 min-max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:, :, :, 9] = train[:, :, :, 9] / 322\n",
    "TEST[:, :, :, 9] = TEST[:, :, :, 9] / 322"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 회전변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation(data):\n",
    "    v1_m_h1 = data[:, :, :, 0] * np.cos(np.pi / 4) - data[:, :, :, 1] * np.sin(np.pi / 4)\n",
    "    v1_p_h1 = data[:, :, :, 0] * np.cos(np.pi / 4) + data[:, :, :, 1] * np.sin(np.pi / 4)\n",
    "    data[:, :, :, 0] = v1_m_h1\n",
    "    data[:, :, :, 1] = v1_p_h1\n",
    "    del v1_m_h1\n",
    "    del v1_p_h1\n",
    "\n",
    "    v2_m_h2 = data[:, :, :, 2] * np.cos(np.pi / 4) - data[:, :, :, 3] * np.sin(np.pi / 4)\n",
    "    v2_p_h2 = data[:, :, :, 2] * np.cos(np.pi / 4) + data[:, :, :, 3] * np.sin(np.pi / 4)\n",
    "    data[:, :, :, 2] = v2_m_h2\n",
    "    data[:, :, :, 3] = v2_p_h2\n",
    "    del v2_m_h2\n",
    "    del v2_p_h2\n",
    "\n",
    "    v4_p_h4_30 = data[:, :, :, 5] * np.cos(np.pi / 4) + data[:, :, :, 6] * np.sin(np.pi / 4)\n",
    "    v4_m_h4_30 = data[:, :, :, 5] * np.cos(np.pi / 4) - data[:, :, :, 6] * np.sin(np.pi / 4)\n",
    "    data[:, :, :, 5] = v4_p_h4_30\n",
    "    data[:, :, :, 6] = v4_m_h4_30\n",
    "\n",
    "    v5_p_h5_30 = data[:, :, :, 7] * np.cos(np.pi / 6) + data[:, :, :, 8] * np.sin(np.pi / 6)\n",
    "    v5_m_h5_30 = data[:, :, :, 7] * np.cos(np.pi / 6) - data[:, :, :, 8] * np.sin(np.pi / 6)\n",
    "    data[:, :, :, 7] = v5_p_h5_30\n",
    "    data[:, :, :, 8] = v5_m_h5_30\n",
    "\n",
    "    del v4_p_h4_30\n",
    "    del v4_m_h4_30\n",
    "    del v5_p_h5_30\n",
    "    del v5_m_h5_30\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rotation(train)\n",
    "TEST = rotation(TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V21KUtyl2lnu"
   },
   "source": [
    "## 5. 모델 학습 및 검증 (Model Tuning & Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oNU-ahud2lnv"
   },
   "source": [
    "### 5.1 모델 1\n",
    "- https://dacon.io/competitions/official/235591/codeshare/1110\n",
    "- GoldBar님 코드에서 루프문을 증가시켰습니다.\n",
    "- 레이어를 깊게 쌓아도 오버피팅 문제가 발생하지 않아 시간을 고려하여 9까지 증가시켰습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_model(shape):\n",
    "    inputs = Input(shape)\n",
    "\n",
    "    bn = BatchNormalization()(inputs)\n",
    "    conv0 = Conv2D(256, kernel_size=1, strides=1, padding='same',\n",
    "                   activation='relu', kernel_initializer='he_normal')(bn)\n",
    "\n",
    "    bn = BatchNormalization()(conv0)\n",
    "    conv = Conv2D(128, kernel_size=2, strides=1, padding='same',\n",
    "                  activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([conv0, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    conv = Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                  activation='relu', kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([concat, conv], axis=3)\n",
    "\n",
    "    # 5에서 9로 증가\n",
    "    for i in range(9):\n",
    "        bn = BatchNormalization()(concat)\n",
    "        conv = Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
    "                      activation='relu', kernel_initializer='he_normal')(bn)\n",
    "        concat = concatenate([concat, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    outputs = Conv2D(1, kernel_size=1, strides=1, padding='same',\n",
    "                     activation='relu', kernel_initializer='he_normal')(bn)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate([train, train_y], -1)\n",
    "\n",
    "train1 = np.rot90(train, 1, (1,2))\n",
    "train2 = np.rot90(train, 2, (1,2))\n",
    "train3 = np.rot90(train, 3, (1,2))\n",
    "train_lr = np.fliplr(train)\n",
    "train_ud = np.flipud(train)\n",
    "\n",
    "train = np.vstack([train, train1])\n",
    "del train1\n",
    "\n",
    "train = np.vstack([train, train2])\n",
    "del train2\n",
    "\n",
    "train = np.vstack([train, train3])\n",
    "del train3\n",
    "\n",
    "train = np.vstack([train, train_lr])\n",
    "del train_lr\n",
    "\n",
    "train = np.vstack([train, train_ud])\n",
    "del train_ud\n",
    "\n",
    "train_y = train[:, :, :, -1].copy()\n",
    "train_y = train_y.reshape(train_y.shape[0], train_y.shape[1], train_y.shape[2], 1)\n",
    "train = train[:,:,:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation 이후, 제가 직접 학습시킨 모델을 로드하면 빠르게 submission을 확인할 수 있습니다.\n",
    "\n",
    "# model = resnet_model(train.shape[1:])\n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "# model.load_weights('model1.h5')\n",
    "# res = model.predict(TEST)\n",
    "# result = (np.exp(res)-1) / 1\n",
    "# submission = pd.read_csv('data/sample_submission.csv')\n",
    "# submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "# submission.to_csv('model1_resnet.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_y, test_y = train_test_split(train, train_y, test_size=0.025, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_number = 0\n",
    "history = []\n",
    "scores = []\n",
    "\n",
    "# 많은 데이터 셋으로 학습시키기 위해 FOLD를 100으로 설정했습니다. Fold 1 중간에 Stop시켰기에 break 조건을 넣어놨습니다.\n",
    "FOLD = 100\n",
    "k_fold = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(train, train_y):\n",
    "    x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "\n",
    "    model = resnet_model(train.shape[1:])\n",
    "    model.summary()\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "\n",
    "    es = EarlyStopping(patience=9, verbose=1)\n",
    "    mc = ModelCheckpoint(f'model1_best_{model_number}.h5', save_best_only=True, verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.8, min_lr=0.0001)\n",
    "    csv_logger = CSVLogger(f'training_{model_number}.csv')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs = 53, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp, csv_logger])\n",
    "    \n",
    "    # 에폭 53번까지 돌리다가 실수로 중단시켜\n",
    "    break\n",
    "    \n",
    "# 14번 더 돌렸습니다. (mae가 개선되지 않아 14번 돌리다가 도중에 중단시켰습니다.)\n",
    "model.fit(x_train, y_train, epochs = 14, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp, csv_logger])\n",
    "model.load_weights(f'model1_best_{model_number}.h5')\n",
    "res = model.predict(TEST)\n",
    "result = (np.exp(res)-1)\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "submission.to_csv('model1_resnet.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 모델 2 - inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://norman3.github.io/papers/docs/google_inception.html  \n",
    "인셉션 v3 모델 앞 부분에서 착안하여 모델을 만들어 보았습니다.\n",
    "\n",
    "![](imgs/inception.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception(shape_, LOOP):\n",
    "    \n",
    "    input_ = Input(shape=shape_)\n",
    "    activation_ = 'relu'\n",
    "    \n",
    "    bn = BatchNormalization()(input_)\n",
    "    conv0 = Conv2D(256, kernel_size=1, strides=1, padding='same',\n",
    "                   activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "    bn = BatchNormalization()(conv0)\n",
    "    conv = Conv2D(128, kernel_size=2, strides=1, padding='same',\n",
    "                  activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([conv0, conv], axis=3)\n",
    "\n",
    "    bn = BatchNormalization()(concat)\n",
    "    conv = Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
    "                  activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "    concat = concatenate([concat, conv], axis=3)\n",
    "    \n",
    "    for i in range(LOOP):\n",
    "        bn = BatchNormalization()(concat)\n",
    "        x_1 = Conv2D(32, 1, padding='same', activation=activation_)(bn)\n",
    "\n",
    "        x_2 = Conv2D(32, 1, padding='same', activation=activation_)(bn)\n",
    "        x_2 = Conv2D(32, 3, padding='same', activation=activation_)(x_2)\n",
    "\n",
    "        x_3 = Conv2D(32, 1, padding='same', activation=activation_)(bn)\n",
    "        x_3 = Conv2D(32, 3, padding='same', activation=activation_)(x_3)\n",
    "        x_3 = Conv2D(32, 3, padding='same', activation=activation_)(x_3)\n",
    "\n",
    "        x_4 = AveragePooling2D(\n",
    "            pool_size=(3, 3), strides=1, padding='same')(bn)\n",
    "        x_4 = Conv2D(32, 1, padding='same', activation=activation_)(x_4)\n",
    "\n",
    "        concat = concatenate([x_1, x_2, x_3, x_4])\n",
    "    \n",
    "    bn = BatchNormalization()(concat)\n",
    "\n",
    "    outputs = Conv2D(1, kernel_size=1, strides=1, padding='same',\n",
    "                     activation=activation_, kernel_initializer='he_normal')(bn)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 augmentation (위와 동일하기에 생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation 이후, 제가 직접 학습시킨 모델을 로드하면 빠르게 submission을 확인할 수 있습니다.\n",
    "# model = inception(train.shape[1:] , 5) \n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "# model.load_weights('model2.h5')\n",
    "# res = model.predict(TEST)\n",
    "# result = (np.exp(res)-1) / 1\n",
    "# submission = pd.read_csv('data/sample_submission.csv')\n",
    "# submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "# submission.to_csv('model2_inception.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 0\n",
    "history = []\n",
    "scores = []\n",
    "\n",
    "# 위에서 설명했듯이 많은 데이터 셋으로 학습시키기 위해 FOLD를 100으로 설정했습니다. Fold 1 중간에 Stop시켰기에 break 조건을 넣어놨습니다.\n",
    "FOLD = 100\n",
    "k_fold = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(train, train_y):\n",
    "    x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "\n",
    "    model = inception(train.shape[1:] , 5)\n",
    "    model.summary()\n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "    \n",
    "    es = EarlyStopping(patience=9, verbose=1)\n",
    "    mc = ModelCheckpoint(f'model2_best_{model_number}.h5', save_best_only=True, verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.8, min_lr=0.0001)\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs = 85, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp])\n",
    "    model.load_weights(f'model2_best_{model_number}.h5')\n",
    "    res = model.predict(TEST)\n",
    "\n",
    "    break\n",
    "    \n",
    "result = (np.exp(res)-1)\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "submission.to_csv('model2_inception.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 모델 3 - deep inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 augmentation\n",
    "\n",
    "대회 마지막 날인만큼 데이터 augmentation을 증가시켰고, layer 깊이도 증가시켰습니다.  \n",
    "앙상블을 제외한 단일 모델로서 가장 결과가 좋았습니다.  \n",
    "augmentation을 증가시키기에 데이터를 처음부터 불러와서 작업을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with open('data/train50.pickle', 'rb') as f:\n",
    "    train = pickle.load(f)\n",
    "    \n",
    "# 0~9번채널만 사용\n",
    "train = train[:, :, :, :10]\n",
    "\n",
    "with open('data/train_y50.pickle', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "train_y = train_y.reshape(train_y.shape[0], 40, 40, 1)\n",
    "\n",
    "  \n",
    "\n",
    "with open('data/test.pickle', 'rb') as f:\n",
    "    TEST = pickle.load(f)\n",
    "TEST = TEST[:, :, :, :10] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.log(train_y+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = channel_sum(train)\n",
    "TEST = channel_sum(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[:, :, :, 9] = train[:, :, :, 9] / 322\n",
    "TEST[:, :, :, 9] = TEST[:, :, :, 9] / 322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rotation(train)\n",
    "TEST = rotation(TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.concatenate([train, train_y], -1)\n",
    "\n",
    "train1 = np.rot90(train, 1, (1,2))\n",
    "train2 = np.rot90(train, 2, (1,2))\n",
    "train3 = np.rot90(train, 3, (1,2))\n",
    "\n",
    "train_lr = np.fliplr(train)\n",
    "train_lr1 = np.rot90(train_lr, 1, (1,2))\n",
    "train_lr2 = np.rot90(train_lr, 2, (1,2))\n",
    "train_lr3 = np.rot90(train_lr, 3, (1,2))\n",
    "\n",
    "train = np.vstack([train, train1])\n",
    "del train1\n",
    "\n",
    "train = np.vstack([train, train2])\n",
    "del train2\n",
    "\n",
    "train = np.vstack([train, train3])\n",
    "del train3\n",
    "\n",
    "train = np.vstack([train, train_lr1])\n",
    "del train_lr1\n",
    "\n",
    "train = np.vstack([train, train_lr2])\n",
    "del train_lr2\n",
    "\n",
    "train = np.vstack([train, train_lr3])\n",
    "del train_lr3\n",
    "\n",
    "train = np.vstack([train, train_lr])\n",
    "del train_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train[:, :, :, -1].copy()\n",
    "train_y = train_y.reshape(train_y.shape[0], train_y.shape[1], train_y.shape[2], 1)\n",
    "train = train[:,:,:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test, train_y, test_y = train_test_split(train, train_y, test_size=0.025, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation 및 피쳐 생성 이후, 제가 직접 학습시킨 모델을 로드하면 빠르게 submission을 확인할 수 있습니다.\n",
    "# model = inception(train.shape[1:] , 7) # loop문을 7번으로 증가.\n",
    "# model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "# model.load_weights('model3.h5')\n",
    "# res = model.predict(TEST)\n",
    "# result = (np.exp(res)-1)\n",
    "# submission = pd.read_csv('data/sample_submission.csv')\n",
    "# submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "# submission.to_csv('model3_deep_inception.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 위에서 설명했듯이 많은 데이터 셋으로 학습시키기 위해 FOLD를 100으로 설정했습니다. Fold 1 중간에 Stop시켰기에 break 조건을 넣어놨습니다.\n",
    "FOLD = 100\n",
    "k_fold = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "model_number = 0\n",
    "\n",
    "for train_idx, val_idx in k_fold.split(train, train_y):\n",
    "    x_train, y_train = train[train_idx], train_y[train_idx]\n",
    "    x_val, y_val = train[val_idx], train_y[val_idx]\n",
    "    print(x_train.shape)\n",
    "\n",
    "    # 루프문 7번으로 증가\n",
    "    model = inception(train.shape[1:] , 7) \n",
    "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "\n",
    "    es = EarlyStopping(patience=9, verbose=1)\n",
    "    mc = ModelCheckpoint(f'model3_best_{model_number}.h5', save_best_only=True, verbose=1)\n",
    "    rlp = ReduceLROnPlateau(monitor='val_loss', patience=4, factor=0.8, min_lr=0.0001)\n",
    "\n",
    "    model.fit(x_train, y_train, epochs = 90, validation_data=(x_val, y_val), verbose=1, batch_size = 64, callbacks = [es, mc, rlp])\n",
    "    model.load_weights(f'model3_best_{model_number}.h5')\n",
    "    res = model.predict(TEST)\n",
    "    \n",
    "    break\n",
    "    \n",
    "result = (np.exp(res)-1)\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.iloc[:,1:] = result.reshape(-1, 1600)\n",
    "submission.to_csv('model3_deep_inception.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 앙상블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation 데이터 셋을 이용하여 점수 개선이 있는지 확인하였고, 점수가 개선되어 각 모델간의 결과를 앙상블하여 제출하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_1 = pd.read_csv('model1_resnet.csv')\n",
    "result_2 = pd.read_csv('model2_inception.csv')\n",
    "result_3 = pd.read_csv('model3_deep_inception.csv')\n",
    "\n",
    "result_1.iloc[:, 1:] = (result_1.iloc[:, 1:] * 0.25) + (result_2.iloc[:, 1:] * 0.25) + (result_3.iloc[:, 1:] * 0.5)\n",
    "\n",
    "result_1.to_csv('endgame_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3BNtFKiZ2ln6"
   },
   "source": [
    "## 6. 결과 및 결언 (Conclusion & Discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMEdF_8z2ln7"
   },
   "source": [
    "- 모델이 오버피팅 되는 경우는 적었습니다. 이에 모델의 layer를 깊게 쌓아 점수가 향상되었습니다.\n",
    "- 레스넷 모델의 경우, augmentation을 적게 했는데, 더 많은 데이터를 바탕으로 모델을 돌리면 점수가 향상되고, 이를 앙상블하면 더 좋은 점수를 기대할 수 있을 것 같습니다.\n",
    "- tree 계열 모델을 만들어 보았으나, 결과가 좋지 않았습니다.\n",
    "- xception, resnet 등의 레이어를 참조하여 모델을 만들어 보았으나 결과가 좋지 않았습니다.\n",
    "- 대회를 준비하신 모든 분들, 대회에 참여하신 모든 분들 고생많았습니다.\n",
    "- 마지막으로 GoldBar 님께 감사하다는 말을 전하고 싶습니다."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AI_S2_코드양식.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
